\chapter{Conclusions and future work}
\label{chapter:conclusions}

\section{Conclusions}

The main goal of this project was to try to obtain experimental evidence supporting our hypothesis that, in an environment with a complex action and observation space, a hierarchical reinforcement learning model can generate agents with performance matching that of traditional models while producing less carbon emissions during training. To test this, we have expanded upon previous work from other students by improving the agents' implementation, increasing the action space while reducing the amount of pre-programmed logic, creating new environments, and improving the observations and reward signals.

We have trained a hierarchical agent and two single agents with different restrictions to compare their performance and training costs. The hierarchical agent was able to easily outperform an single agent with a considerably more costly training process, while the single agent limited to the same training cost as the hierarchical agent wasn't able to properly learn the environment, ending up with a performance worse than random. While the experimental phase wasn't exhaustive and we were limited by time constraints, our results lean heavily in favour of our original hypothesis.

When it comes to the training costs of the agents, we have observed two main benefits for the hierarchical model thanks to its use of sub-agents. The first is that, by training the sub-agents individually, their environments can be finely-tuned to their needs, leading to shorter learning times. We did this for our three sub-agents by creating custom training maps, but specially for the recruit manager for which we created two maps: one to facilitate exploration and a second to complete the training. The second benefit is that, since the sub-agents only need to learn simpler tasks (with smaller action and observation spaces), they can use smaller neural networks than single agents and train for smaller amounts of time. The combined benefit of smaller networks and reduced training time is such that, even when needing to create several smaller agents, the total compute cost is greatly reduced.

Looking back at the subgoals first defined in chapter \ref{chapter:introduction}, we can confidently say that we have achieved the first two: \textbf{Reducing built-in logic in actions} and \textbf{Improving the reward signal}. As for the third one, \textbf{Finding new subtasks}, we tried to find a part of the agent's existing game plan that could be handled by a new sub-agent, but were unable to find one that wasn't redundant or didn't involve dramatically increasing the number of tools that the agent can access, which would be beyond the scope of the project. The existing division of tasks between the sub-agents perfectly encapsulates the macro steps that the agents need to win the game.

\section{Future work}

Our work in this project is not definitive. With more time and resources it could be continued in several ways, examples of which are:

\begin{itemize}
    \item \textbf{Further expand the environment complexity.} Since we believe that an environment with a complex action and state space is one of the elements that allows a hierarchical agent to outshine a single agent, continuing the experiments in a more complex environment could help cement this idea. A way to achieve this could be expanding the actions that the agents can take (such as allowing them to repair damaged buildings, or complete structures left unfinished), increasing the number of units and resources that the agents have access to (introducing vespene gas and advanced units and structures), or even introducing one or both of the remaining playable races, either as opponents or for the agent to learn as well.
    \item \textbf{Improve reward signals.} Even though we have improved the reward signal for the \texttt{Simple\_64} map, we believe there is still room for improvement regarding the rewards of the sub-agents. To facilitate fine-tuning, we used short-term or immediate rewards when possible, but the rewards were not ideal for finding an optimal solution. Devising more accurate reward signals could go a long way to get more efficient policies.
    \item \textbf{Perform more extensive training.} Due to time limitations, the agents' training was not as thorough as could be desired. While we were unable to match the performance of the hierarchical agent using a single agent with a medium or large network, perhaps a better configuration or set of hyperparameters could yield better results. This also includes certain agent configurations such as the action frequency or the temporal extension of the hierarchical agent.
    \item \textbf{Experiment with different hierarchical strategies.} We have focused on the \textit{options} hierarchical strategy for this whole project, but other hierarchical methodologies could be tested to see if they provide the same efficiency benefits. An example would be Feudal HRL, in which a manager agent learns to define sub-tasks and rewards for its worker agents.
    \item \textbf{Improve efficiency measurements.} Lastly it would be beneficial to use hardware and software with better compatibility to be able to take accurate measurements of energy consumption with codecarbon, or to find an alternative way to obtain this data.
\end{itemize}
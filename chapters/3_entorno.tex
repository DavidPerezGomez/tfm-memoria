\chapter{Environment}
\label{chapter:environment}

In this chapter, we describe de characteristics of StarCraft II as a reinforcement learning environment followed by the modification we have made to said environment to adapt it to this project.

\section{StarCraft II}

StarCraft II is a real-time strategy video-game developed by Blizzard Entertainment, Inc in 2010. The game was extremely successful with the casual audience and it spawned a professional competition scene that continues to this day. In the field of machine learning, StarCraft II was first explored in 2017 \cite{Vinyals:2017} and it has since become a frequently tackled game thanks to its complexity and the wide range of challenges that it provides \cite{Tang:2018}.

StarCraft II is played on a mostly flat map with an isometric or birds-eye view. The main game-mode is versus, in which two or more players fight against each other until all but one of the players (or teams) are defeated. Each player needs to collect resources from the map, build structures and recruit and upgrade their army to attack other players and defend themselves. A player is defeated when all their structures have been destroyed. There are three playable races for the players to choose (Terran, Protoss and Zerg), each with their own unique units, structures, upgrade trees and game mechanics.

The game also supports user-created maps and is distributed alongside a map editor with extensive tools to create, edit and script custom maps.

\section{PySC2 library}

PySC2 is a python library developed by the Google Deep Mind team in collaboration with Blizzard \cite{Vinyals:2017}. It wraps the StarCraft II API and provides specifications for agent actions and environment observations, as well as a reward signal, which allow the use of the game as a reinforcement learning environment.

One of the major benefits of this library is that it can run the game tens of times faster than regular game-play. Internally, the game runs at 16 ticks per in-game second. When running the game at normal speed an in-game second matches a real-life second. PySC2 can run the game at much faster speeds, and we can configure how often (in terms of game ticks) it will stop the simulation, provide an observation and wait until our agent selects an action. For example, by setting the configuration value \texttt{step\_mul} to 32, PySC2 will poll our agent every 32 game ticks, which means the agent will act once every two in-game seconds.

\subsubsection*{Observation}

Every step, PySC2 supplies all the information about the current state of the game, both visually and through structured data. On the visual side, the agent has access to 27 feature layers, which are pixelated RGB representations of either the game screen or the mini-map (see figure \ref{fig:feature_layers}). Each feature layer displays different aspects of the game, such as the terrain height-map, the revealed parts of the map, units of a specific type or that belong to a specific player, etc. These layers are meant to allow agents to play the game using only (or mainly) visual information similar to that which a human player would have.

Additionally, the agents also have access to structured data about the state of the game, in the form of several tensors. These tensors contain numerical information about the players, resources and units in the map, as well as accumulated data such as total damage dealt or total resources spent.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figs/feature_layers.png}
    \caption{PySC2 feature layer view}
    \label{fig:feature_layers}
\end{figure}

\subsubsection*{Action space}

PySC2 allows the agents to interact with the game by calling functions that represent game actions that a regular player could take. These functions come in two forms. One type of function mimics the way a player would interact with the game, mostly by clicking the game area or the mini-map or pressing hot-keys. This includes clicking on the screen to select a unit, dragging a rectangle to select multiple units, clicking again to issue and order to move or attack, dragging the mouse to move the camera, etc.

The second type of function, called \say{raw function}, encodes higher level actions, such as attacking a specific point with one or more units, building a structure with a specific worker or sending a worker to gather resources. With these, the agent doesn't need to select units before giving the command, or move the camera to be able to see the area where it wants to build or attack. The library handles all the low level \say{basic} actions related to game-control that would combine to form a \say{raw function}.

\subsubsection*{Reward signal}

For each observation in a standard versus map, PySC2 provides two different rewards. One simply reflects the win or lose status at the end of a game: 1 for victory, -1 for defeat and 0 for draw (in case of a time-out). Every step during the game before the end of the match has a reward of 0. While this reward matches the desired behavior of the agents (we simply care about winning and not losing) it can be too sparse for the agents to properly learn, since the only receive one instance of reward after every game, which can easily take hundreds or thousands of steps.

For that reason, a second, dense reward is also included. This reward matches the internal score that the game assigns to each player. This score is calculated based on the amount of resources collected, upgrades researched, and unit and structures built. It is not a perfect metric to determine victory, but it serves as a decent estimator for which player is in a better position.

\subsection{Mini-games}

PySC2 comes with a selection of mini-games, created with the StarCraft II map editor, to serve as small environments with simpler objectives. These maps often have different reward signal to better match the goal of each mini-games.

\section{Simplified environment}

Since our goal is not to create an agent as proficient as possible in the game of StarCraft II and the scope of this project is relatively modest, we have decided to simplify the environment in a way that suits our needs.

\subsection{Race selection and technology progression}

Firstly, we have limited the race selection during all of our experiments to only one of the three races. That includes both our agents and the opponents they will fight against. The reason for this is that all three races are extremely different in many aspects of play. They create different units and structures, follow different progressions, interact with resources in different ways and are better suited for different strategies.

Similarly, we have also decided to have the agents work with only a fraction of the entire toolset available for the race. This is because the technology progression tree of structures, units and upgrades is far too expansive and nuanced for our scope.

Taking this into account, we have chosen the Terrans as the race to focus on since it can be considered the most simple and straight-forward of the three, and the one that functions best with a limited progression. The agents will have access to the following units and structures:

\begin{itemize}
    \item \textbf{Command center:} Structure used to recruit new SCVs and necessary to collect minerals, the main resource of the game. Every player begins the game with one command center next to a patch of mineral crystals.
    \item \textbf{SCV:} Worker unit used to build all Terran structures and to collect minerals and take them to a command center. Every player begins the game with eight SCVs.
    \item \textbf{Supply depot:} Structure that increases the maximum supply of the player, which determines the amount of unit the player can have. At least one supply depot is required to build a barracks.
    \item \textbf{Barracks:} Structure used to recruit new marines.
    \item \textbf{Marine:} Ranged combat unit capable of attacking and destroying enemy units and structures.
\end{itemize}

These units, although limited for a normal game of StarCraft II, are enough for the agents to perform the basic, general steps required to win the game. The expected strategy would be:

\begin{enumerate}
    \item Gather minerals with the starting SCVs and build new SCVs to speed up the process.
    \item Optionally, build a new command center near another source of minerals and more workers to improve the resource economy.
    \item Build a supply depot.
    \item Build a barracks.
    \item Start recruiting marines while continuing to build supply depots (to allow for more marines) and barracks (to recruit marines faster).
    \item Attack the enemy base with the marines.
\end{enumerate}

\subsection{Action space}
\label{sec:action_space}

To limit the action space to a more manageable level and to align with the limited progression described previously, we have decided on the following list of action for the agents:

\begin{itemize}
    \item \textbf{\texttt{NO\_OP:}} No action is taken for the current step.
    \item \textbf{\texttt{HARVEST\_MINERALS:}} Order a random idle worker to gather minerals from the closes mineral deposit.
    \item \textbf{\texttt{BUILD\_COMMAND\_CENTER:}} Order a random idle or harvesting worker to build a command center. The agent can only have up to three command centers.
    \item \textbf{\texttt{RECRUIT\_SCV\_0:}} Recruit an SCV on the first command center.
    \item \textbf{\texttt{RECRUIT\_SCV\_1:}} Recruit an SCV on the second command center.
    \item \textbf{\texttt{RECRUIT\_SCV\_2:}} Recruit an SCV on the third command center.
    \item \textbf{\texttt{BUILD\_SUPPLY\_DEPOT:}} Order a random idle or harvesting worker to build a supply depot. The agent can only have up to 24 supply depots.
    \item \textbf{\texttt{BUILD\_BARRACKS:}} Order a random idle or harvesting worker to build a barracks. The agent can only have up to four barracks.
    \item \textbf{\texttt{RECRUIT\_MARINE:}} Recruit a marine on the barracks with the shortest queue.
    \item \textbf{\texttt{ATTACK\_CLOSEST\_BUILDING:}} Attack with all marines to the enemy building that is closest to the average position of all marines.
    \item \textbf{\texttt{ATTACK\_CLOSEST\_WORKER:}} Attack with all marines to the enemy worker that is closest to the average position of all marines.
    \item \textbf{\texttt{ATTACK\_CLOSEST\_ARMY:}} Attack with all marines to the enemy combat unit that is closest to the average position of all marines.
    \item \textbf{\texttt{ATTACK\_BUILDINGS:}} Attack with all marines to the enemy building that is closest to the average position of all enemy buildings.
    \item \textbf{\texttt{ATTACK\_WORKERS:}} Attack with all marines to the enemy worker that is closest to the average position of all enemy workers.
    \item \textbf{\texttt{ATTACK\_ARMY:}} Attack with all marines to the enemy combat unit that is closest to the average position of all enemy combat units.
\end{itemize}

For all building types, we have predefined all the position where they can be built for each map. The versus map we will use only supports up to two additional bases, which is why we limit the command centers to three (including the one each player starts with). The supply depots are limited to 24 since that is the amount that will bring the supply resource to its maximum. Any supply depot beyond the 24\textsuperscript{th} will have no effect. The limit of four barracks is somewhat arbitrary. Four is enough to build marines at a sufficient speed to win the game.

When it comes to recruiting new units, we have allowed the agent to choose in which of its command centers to build the SCVs. Because assigning too many workers to harvest on the same mineral patch has diminishing returns, and since the harvest order always points the selected worker to the closest mineral deposit, this give the agent a certain amount of control on which of the bases' resource economy to develop.

As for the marines, all barracks are built next to the main base and close to each other, so there is no point in differentiating between them.

\subsection{Observation}

Even though the structured observation provided by PySC2 has a predictable form, it is variable enough that it cannot be used as-is as the input for a neural network. For that reason, and for the fact that it contains considerably more information than our agent could possible need, we have created our own observation data structure to represent the state of the game. Our observation contains mostly data obtained from PySC2's observation, plus some additional data that we calculate. The complete list of fields and their description can be found in appendix \ref{app:observation}, but some highlights include:

\begin{itemize}
    \item The amount of resources the agent has.
    \item The number of each building type the agent has.
    \item The number of workers and marines the agent has.
    \item The number of workers that are idle, building or harvesting and each command center.
    \item The distance from the marines to various targets.
    \item The list of actions the agent can successfully take on the current step.
    \item The enemy's total army health.
\end{itemize}

We make no use of the visual information provided by the feature layers.

\subsection{Reward signal}

The reward signals we have chosen will vary depending on the map and its objective. They are explained in detail in section \ref{sec:mini-games}.

\section{Custom scenarios}
\label{sec:mini-games}

We found that the mini-games distributed with PySC2 didn't fit with the needs and limitations of our different agents, which are explained in chapter \ref{chapter:implementation}. For that reason, we have used the StarCraft II map editor to create our own custom maps.

\subsubsection*{\texttt{BuildMarinesRandom}}

The objective of this mini-game is to recruit as many marines as possible in a limited time-frame. The agent start with one command center and 16 SCVs next to a mineral patch. Additionally, to favour the exploration process, the scenario can randomly start with one supply depot already built, or one supply depot and one to four barracks already built. The more advanced the starting base, the less likely it is.

The map has a fixed duration of 10 minutes. The score for each step is the amount of minerals spent on recruiting marines since the last step. The agent has to build supply depots, barracks and marines in the most optimal way possible to maximize the score.

$$
r(t) = \texttt{spending\_marines}_{t} - \texttt{spending\_marines}_{t-1}
$$

\subsubsection*{\texttt{BuildMarinesFixed}}

This map is identical to \texttt{BuildMarinesRandom} except that it always start with only the command center and the SVCs. It is used for inference purposes.

\subsubsection*{\texttt{CollectMineralsRandom}}

The objective of this mini-game is to harvest as many minerals as possible in a limited time-frame. The agent start with one command center and 16 SCVs next to a mineral patch. The map includes two additional mineral patches to allow for up to two more bases to be built. Additionally, to favour the exploration process, the scenario can randomly start with one or two extra command centers already built. The more starting command centers, the less likely it is.

The map has a fixed duration of 12 minutes. The score for each step is the amount of minerals gained or lost since the last step. The agent has to build SCVs, supply depots, and command centers in the most optimal way possible to maximize the score.

$$
r(t) = \texttt{minerals}_{t} - \texttt{minerals}_{t-1}
$$

\subsubsection*{\texttt{CollectMineralsFixed}}

This map is identical to \texttt{CollectMineralsRandom} except that it always start with only one command center and the SVCs. It is used for inference purposes.

\subsubsection*{\texttt{DefeatBases}}

The objective of this mini-game is to defeat two small enemy bases. The agent starts with a group of marines of variable size placed a random point close to the center of the map. The enemy bases consist of three supply depots, a small random number of SCVs and a small random number of marines, randomly positioned on opposite corners of the map. The enemy marines are programmed to attack any units that damage nearby buildings and the SCVs are programmed to repair nearby buildings. The number of ally and enemy marines is set up so that the allies can destroy both bases reliably if played correctly.

The map ends when all enemy buildings have been destroyed or after 3 minutes. The score for each step is the change in health score since the last step, with an additional flat penalty each step to encourage faster episodes. The agent has to prioritize targeting enemy marines, then SCVs and then buildings, and to focus on a single base at a time to maximize the score.

$$
\texttt{health\_score} = \texttt{health\_allies} \cdot 1.1 - \texttt{health\_enemies}
$$
$$
r(t) = \texttt{health\_score}_{t} - \texttt{health\_score}_{t-1} - \texttt{step\_cost}
$$

\subsubsection*{\texttt{Simple64}}

This is the only custom map included in PySC2 that we used without any changes. It is a two-player map, classified as a \say{melee} map instead of a mini-game, meaning the objective is simply to defeat the enemy player. Each player starts, randomly, in one of two opposite corners of a symmetric battlefield with a command center and 12 SCVs next to a mineral patch. This emulates a real game of StarCraft II.

The game ends when one of the player has been defeated (by losing all of their structures). It can also end in a draw if enough time passes with none of the players being able to gather more resources, produce more units or destroy enemy units. The score for each step is the change in a custom player score since the last step. The player score is calculated with the mineral value of all units of that player factored by the health or build progress of the unit.

$$
r(t) = (\texttt{ally\_score}_{t} - \texttt{enemy\_score}_{t}) - (\texttt{ally\_score}_{t-1} - \texttt{enemy\_score}_{t-1})
$$
%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Jordi Casas Roma at 2018-05-29 18:21:49 +0200 

%% Saved with string encoding Unicode (UTF-8) 

@misc{Caceres:2024,
  title  = {Reducing Computational Costs in Deep Reinforcement Learning for Real-Time Strategy Games},
  author = {Almir Cáceres Berraquero and Luis Esteve Elfau and Ismael Benito Altamirano},
  year   = {2024}
}

@misc{Gimenez:2024,
  title  = {Improving the efficiency of a Starcraft II (multi)agent training using Feudal/Hierarchical Reinforcement Learning},
  author = {Albert Giménez Morales and Luis Esteve Elfau and Esther Ibáñez Marcelo},
  year   = {2024}
}

@misc{Vinyals:2017,
  title  = {StarCraft II: A New Challenge for Reinforcement Learning},
  author = {Oriol Vinyals and Timo Ewalds and Sergey Bartunov and Petko Georgiev and Alexander Sasha Vezhnevets and Michelle Yeo and Alireza Makhzani and Heinrich Küttler and John Agapiou and Julian Schrittwieser and John Quan and Stephen Gaffney and Stig Petersen and Karen Simonyan and Tom Schaul and Hado van Hasselt and David Silver and Timothy Lillicrap and Kevin Calderone and Paul Keet and Anthony Brunasso and David Lawrence and Anders Ekermo and Jacob Repp and Rodney Tsing},
  year   = {2017},
  doi    = {10.48550/arXiv.1708.04782}
}

@misc{Nestor:2024,
  title  = {Artificial Intelligence Index Report 2024},
  author = {Nestor Maslej and Loredana Fattorini and Raymond Perrault and Vanessa Parli and Anka Reuel and Erik Brynjolfsson and John Etchemendy and Katrina Ligett and Terah Lyons and James Manyika and Juan Carlos Niebles and Yoav Shoham and Russell Wald and Jack Clark},
  year   = {2024},
  doi    = {10.48550/arXiv.2405.19522}
}

@article{Schwartz:2019,
  title  = {Green AI},
  author = {Roy Schwartz and Jesse Dodge and Noah A. Smith and Oren Etzioni},
  year   = {2019},
  doi    = {10.48550/arXiv.1907.10597}
}

@article{Amodei:2018,
  title  = {AI and compute},
  author = {Dario Amodei and Danny Hernandez},
  year   = {2018},
  url    = {https://openai.com/index/ai-and-compute/}
}

@inproceedings{Jaime:2022,
  author    = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  booktitle = {2022 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Compute Trends Across Three Eras of Machine Learning},
  year      = {2022},
  pages     = {1-8},
  keywords  = {Deep learning;Training;Machine learning algorithms;Computational modeling;Neural networks;Market research;History;machine learning;artificial intelligence;deep learning;computational efficiency;AI accelerators;backpropagation;high performance computing},
  doi       = {10.1109/IJCNN55064.2022.9891914}
}

@article{Strubell:2019,
  title  = {Energy and Policy Considerations for Deep Learning in NLP},
  author = {Emma Strubell and Ananya Ganesh and Andrew McCallum},
  year   = {2019},
  doi    = {10.48550/arXiv.1906.02243}
}

@inproceedings{Zheng:2020,
  author    = {Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {6708--6718},
  publisher = {Curran Associates, Inc.},
  title     = {On Efficiency in Hierarchical Reinforcement Learning},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4a5cfa9281924139db466a8a19291aff-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{Barto:2003,
  title     = {Recent advances in hierarchical reinforcement learning},
  author    = {Andrew G Barto and Sridhar Mahadevan},
  journal   = {Discrete event dynamic systems},
  volume    = {13},
  pages     = {341--379},
  year      = {2003},
  publisher = {Springer},
  doi       = {10.1023/A:1025696116075}
}

@article{Al-Emran:2015,
  author  = {Mostafa Al-Emran},
  title   = {Hierarchical Reinforcement Learning: A Survey},
  year    = {2015},
  journal = {International Journal of Computing and Digital Systems},
  volume  = {4},
  issue   = {2},
  doi     = {10.12785/IJCDS/040207}
}

@article{Pateria:2021,
  author    = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  title     = {Hierarchical Reinforcement Learning: A Comprehensive Survey},
  year      = {2021},
  publisher = {Association for Computing Machinery},
  volume    = {54},
  number    = {5},
  doi       = {10.1145/3453160},
  journal   = {ACM Comput. Surv.}
}

@article{Vezhnevets:2017,
  title  = {FeUdal Networks for Hierarchical Reinforcement Learning},
  author = {Alexander Sasha Vezhnevets and Simon Osindero and Tom Schaul and Nicolas Heess and Max Jaderberg and David Silver and Koray Kavukcuoglu},
  year   = {2017},
  doi    = {10.48550/arXiv.1703.01161}
}

@misc{UN:Goal12,
  title  = {Goal 12: Ensure sustainable consumption and production patterns},
  author = {{United Nations}},
  url    = {https://www.un.org/sustainabledevelopment/sustainable-consumption-production/},
  note   = {Accessed: 2024-10-10}
}

@misc{UN:Goal13,
  title  = {Goal 13: Take urgent action to combat climate change and its impacts},
  author = {{United Nations}},
  url    = {https://www.un.org/sustainabledevelopment/climate-change/},
  note   = {Accessed: 2024-10-10}
}

@article{Kaelbling:1996,
  author  = {Leslie Pack Kaelbling and Michael L. Littman and Andrew W. Moore},
  title   = {Reinforcement Learning: A Survey},
  year    = {1996},
  journal = {Journal of Articial Intelligence Research},
  volume  = {5},
  doi     = {10.1613/jair.301}
}

@article{Bowling:2023,
  title  = {Settling the Reward Hypothesis},
  author = {Michael Bowling and John D. Martin and David Abel and Will Dabney},
  year   = {2023},
  doi    = {10.48550/arXiv.2212.10420}
}

@book{Sutton:2018,
  author    = {Richard S. Sutton and Andrew G. Barto},
  title     = {Reinforcement Learning: An Introduction},
  publisher = {MIT Press},
  year      = {2018}
}

@article{Watkins:1992,
  author  = {Christopher J. C. H. Watkins and Peter Dayan},
  title   = {Q-learning},
  year    = {1992},
  journal = {Machine Learning},
  volume  = {8},
  pages   = {279--292},
  doi     = {10.1007/BF00992698}
}

@article{Mnih:2013,
  title  = {Playing Atari with Deep Reinforcement Learning},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  year   = {2013},
  doi    = {10.48550/arXiv.1312.5602}
}

@article{Mnih:2015,
  title   = {Human-level control through deep reinforcement learning},
  author  = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  year    = {2015},
  journal = {Nature},
  volume  = {518},
  pages   = {529--533},
  doi     = {10.1038/nature14236}
}

@book{Bellman:1957,
  title     = {Dynamic Programming},
  author    = {Richard Bellman and Rand Corporation and Karreman Mathematics Research Collection},
  series    = {Rand Corporation research study},
  year      = {1957},
  publisher = {Princeton University Press}
}

@inproceedings{Bianchi:2004,
  author    = {Reinaldo A. C. Bianchi and Carlos H. C. Ribeiro and Anna H. R. Costa},
  editor    = {Bazzan, Ana L. C. and Labidi, Sofiane},
  title     = {Heuristically Accelerated Q--Learning: A New Approach to Speed Up Reinforcement Learning},
  booktitle = {Advances in Artificial Intelligence -- SBIA 2004},
  year      = {2004},
  publisher = {Springer Berlin Heidelberg},
  pages     = {245--254},
  doi       = {10.1007/978-3-540-28645-5_25}
}

@inproceedings{Cheng:2021,
  author    = {Ching-An Cheng and Andrey Kolobov and Adith Swaminathan},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {13550--13563},
  publisher = {Curran Associates, Inc.},
  title     = {Heuristic-Guided Reinforcement Learning},
  volume    = {34},
  year      = {2021},
  doi       = {10.48550/arXiv.2106.02757}
}

@article{Botvinick:2019,
  title   = {Reinforcement Learning, Fast and Slow},
  author  = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  doi     = {10.1016/j.tics.2019.02.006},
  number  = {5},
  volume  = {23},
  month   = {May},
  year    = {2019},
  journal = {Trends in cognitive sciences},
  pages   = {408--422}
}


@inproceedings{Pritzel:2017,
  title     = {Neural Episodic Control},
  author    = {Alexander Pritzel and Benigno Uria and Sriram Srinivasan and Adri{\`a} Puigdom{\`e}nech Badia and Oriol Vinyals and Demis Hassabis and Daan Wierstra and Charles Blundell},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {2827--2836},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  doi       = {10.48550/arXiv.1703.01988}
}

@misc{Wang:2017,
  title  = {Learning to reinforcement learn},
  author = {Jane X Wang and Zeb Kurth-Nelson and Dhruva Tirumala and Hubert Soyer and Joel Z Leibo and Remi Munos and Charles Blundell and Dharshan Kumaran and Matt Botvinick},
  year   = {2017},
  doi    = {10.48550/arXiv.1611.05763}
}

@misc{Dietterich:1999,
  title  = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
  author = {Thomas G. Dietterich},
  year   = {1999},
  doi    = {10.48550/arXiv.cs/9905014}
}

@misc{Vezhnevets:2017,
  title  = {FeUdal Networks for Hierarchical Reinforcement Learning},
  author = {Alexander Sasha Vezhnevets and Simon Osindero and Tom Schaul and Nicolas Heess and Max Jaderberg and David Silver and Koray Kavukcuoglu},
  year   = {2017},
  doi    = {10.48550/arXiv.1703.01161}
}

@misc{Nachum:2019,
  title  = {Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?},
  author = {Ofir Nachum and Haoran Tang and Xingyu Lu and Shixiang Gu and Honglak Lee and Sergey Levine},
  year   = {2019},
  doi    = {10.48550/arXiv.1909.10618}
}

@article{Duan:2020,
  author  = {Duan, Jingliang and Eben Li, Shengbo and Guan, Yang and Sun, Qi and Cheng, Bo},
  title   = {Hierarchical reinforcement learning for self-driving decision-making without reliance on labelled driving data},
  journal = {IET Intelligent Transport Systems},
  volume  = {14},
  number  = {5},
  pages   = {297--305},
  doi     = {10.1049/iet-its.2019.0317},
  year    = {2020}
}

@article{Qi:2022,
  title   = {Hierarchical reinforcement learning based energy management strategy for hybrid electric vehicle},
  journal = {Energy},
  volume  = {238},
  pages   = {121703},
  year    = {2022},
  doi     = {10.1016/j.energy.2021.121703},
  author  = {Chunyang Qi and Yiwen Zhu and Chuanxue Song and Guangfu Yan and Feng Xiao and  {Da wang} and Xu Zhang and Jingwei Cao and Shixin Song}
}

@article{Tan:2024,
  author  = {Zhentao Tan and Yadong Mu},
  title   = {Hierarchical reinforcement learning for chip-macro placement in integrated circuit},
  journal = {Pattern Recognition Letters},
  volume  = {179},
  pages   = {108--114},
  year    = {2024},
  doi     = {10.1016/j.patrec.2024.02.002}
}

@article{Li:2023,
  author    = {Li, Lan and He, Fazhi and Fan, Rubin and Fan, Bo and Yan, Xiaohu},
  title     = {3D reconstruction based on hierarchical reinforcement learning with transferability},
  journal   = {Integrated Computer-Aided Engineering},
  publisher = {IOS Press},
  volume    = {30},
  number    = {4},
  pages     = {327--339},
  year      = {2023},
  doi       = {10.3233/ICA-230710}
}